{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%uv pip install -U bitsandbytes accelerate\n",
        "%uv pip install -U transformers diffusers\n",
        "%uv pip install transformers==4.48.0\n",
        "%uv pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token \"Your_Huggingface_Token_Here\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from diffusers import FluxKontextPipeline\n",
        "import torch\n",
        "\n",
        "# Load base model\n",
        "pipe = FluxKontextPipeline.from_pretrained(\n",
        "    \"black-forest-labs/FLUX.1-Kontext-dev\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ").to(\"cuda\")\n",
        "lora_path = \"./\"  # folder containing the safetensor file\n",
        "weight_name = \"eyeQ1ZtOpNxv-bmOE7iV6_adapter_model.safetensors\"\n",
        "\n",
        "# Load the LoRA weights\n",
        "pipe.load_lora_weights(\n",
        "    lora_path,\n",
        "    weight_name=weight_name,\n",
        "    adapter_name=\"my_lora\"\n",
        ")\n",
        "\n",
        "# Activate the adapter\n",
        "pipe.set_adapters([\"my_lora\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pkill ngrok || true\n",
        "!ngrok config add-authtoken \"add_your_ngrok_token_here\"\n",
        "!pip install fastapi uvicorn pyngrok torch diffusers transformers pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import io\n",
        "import base64\n",
        "import requests\n",
        "import torch\n",
        "from fastapi import FastAPI, Form\n",
        "from fastapi.responses import JSONResponse\n",
        "from pyngrok import ngrok\n",
        "from diffusers import FluxKontextPipeline\n",
        "from diffusers.utils import load_image\n",
        "from PIL import Image\n",
        "import uvicorn\n",
        "\n",
        "# ==============================================================\n",
        "# üß© Initialize model once at startup\n",
        "# ==============================================================\n",
        "\n",
        "print(\"üîß Loading Flux-Kontext model and LoRA...\")\n",
        "\n",
        "pipe = FluxKontextPipeline.from_pretrained(\n",
        "    \"black-forest-labs/FLUX.1-Kontext-dev\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Path to your LoRA adapter\n",
        "lora_path = \"./\"  # Folder containing the safetensors file\n",
        "weight_name = \"eyeQ1ZtOpNxv-bmOE7iV6_adapter_model.safetensors\"\n",
        "\n",
        "# Load and activate the adapter\n",
        "pipe.load_lora_weights(\n",
        "    lora_path,\n",
        "    weight_name=weight_name,\n",
        "    adapter_name=\"my_lora\"\n",
        ")\n",
        "pipe.set_adapters([\"my_lora\"])\n",
        "\n",
        "print(\"‚úÖ Model + LoRA loaded successfully.\")\n",
        "\n",
        "# ==============================================================\n",
        "# üåê FastAPI + ngrok setup\n",
        "# ==============================================================\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "async def generate_image(\n",
        "    image_url: str = Form(...),\n",
        "    prompt: str = Form(...),\n",
        "    guidance_scale: float = Form(2.5),\n",
        "    num_inference_steps: int = Form(30),\n",
        "    resolution_mode: str = Form(\"2:3\")\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate an image from a given image_url and prompt using Flux-Kontext + LoRA.\n",
        "    Returns a Base64-encoded PNG in JSON.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüì• New request received:\\nPrompt: {prompt}\\nImage: {image_url}\\nResolution: {resolution_mode}\")\n",
        "\n",
        "        # ----------------------------------------------------------\n",
        "        # Load input image\n",
        "        # ----------------------------------------------------------\n",
        "        input_image = load_image(image_url).convert(\"RGB\")\n",
        "\n",
        "        # ----------------------------------------------------------\n",
        "        # Handle resolution / aspect ratio\n",
        "        # ----------------------------------------------------------\n",
        "        if resolution_mode == \"2:3\":\n",
        "            width, height = 768, 1152\n",
        "        elif resolution_mode == \"3:2\":\n",
        "            width, height = 1152, 768\n",
        "        elif resolution_mode == \"1:1\":\n",
        "            width, height = 1024, 1024\n",
        "        else:\n",
        "            width, height = input_image.size  # fallback\n",
        "\n",
        "        input_image = input_image.resize((width, height), Image.LANCZOS)\n",
        "\n",
        "        # ----------------------------------------------------------\n",
        "        # Generate image with LoRA applied\n",
        "        # ----------------------------------------------------------\n",
        "        result = pipe(\n",
        "            image=input_image,\n",
        "            prompt=prompt,\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            width=width,\n",
        "            height=height\n",
        "        ).images[0]\n",
        "\n",
        "        # ----------------------------------------------------------\n",
        "        # Convert to Base64 for JSON response\n",
        "        # ----------------------------------------------------------\n",
        "        buffered = io.BytesIO()\n",
        "        result.save(buffered, format=\"PNG\")\n",
        "        img_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "        print(\"‚úÖ Image generated successfully.\")\n",
        "        return JSONResponse(content={\"image_base64\": img_base64})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\", file=sys.stderr)\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# üöÄ Run the server with ngrok\n",
        "# ==============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Start ngrok tunnel\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"\\nüöÄ Public endpoint: {public_url.public_url}/generate\\n\")\n",
        "\n",
        "    # Launch FastAPI server\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
